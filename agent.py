import os
import asyncio
from dotenv import load_dotenv
from typing import TypedDict, Annotated, List, Any
from operator import add
from langgraph.graph import StateGraph, START, END, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import AnyMessage, AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from utils_epub import parse_epub

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")

# ===
# basic function
# ===
# def _format_docs(docs) -> str:
#     # å°‡æœå°‹åˆ°çš„ Documents åˆä½µæˆå–®ä¸€ context å­—ä¸²
#     return "\n\n".join(d.page_content for d in docs)

class EpubChatState(TypedDict):
    messages: Annotated[List[AnyMessage], add_messages()]
    epub_path: str
    text_chunks: Annotated[List[str], add]
    summary: str
    vectorstore: Any

class EpubChatAgent:
    def __init__(self):
        self.llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=LLM_MODEL_NAME)
        self.embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)

    async def create_agent(self):
        # ===
        # node function
        # ===

        def parse_epub_node(state: EpubChatState):
            """è§£æ EPUB ä¸¦åˆ‡å‰²æˆ chunks"""
            print("æ­£åœ¨è§£æ EPUB...")
            texts = parse_epub(state["epub_path"])
            splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100)
            chunks = splitter.split_text("\n".join(texts))
            return {"text_chunks": chunks}
        
        def summarize_or_index_node(state: EpubChatState):
            """æ‘˜è¦ï¼ˆå…ˆå–å‰ 10 æ®µï¼‰èˆ‡å»ºç«‹å‘é‡ç´¢å¼•"""
            print("ç”¢ç”Ÿæ‘˜è¦ & å»ºç«‹å‘é‡ç´¢å¼• ...")

            prompt = ChatPromptTemplate.from_template("è«‹ç”¨ç¹é«”ä¸­æ–‡ç‚ºä»¥ä¸‹å…§å®¹ç”Ÿæˆæ‘˜è¦ï¼š\n\n{text}")
            chain = prompt | self.llm | StrOutputParser()

            summaries = []
            for chunk in state["text_chunks"][:10]:
                s = chain.invoke({"text": chunk}).strip()
                if s:
                    summaries.append(s)
            summary = "\n".join(summaries)

            store = FAISS.from_texts(state["text_chunks"], embedding=self.embedding)

            print("æ‘˜è¦èˆ‡ç´¢å¼•å®Œæˆ")
            return {"summary": summary, "vectorstore": store}
        
        def qa_node(state: EpubChatState):
            """åŸºæ–¼æª¢ç´¢çš„å•ç­”ï¼ˆä¿®æ­£ç‰ˆï¼Œé˜²æ­¢ list | list éŒ¯èª¤ï¼‰"""
            print("ğŸ’¬ å•ç­”ä¸­ ...")

            retriever = state["vectorstore"].as_retriever(search_kwargs={"k": 5})

            prompt = ChatPromptTemplate.from_template(
                "ä»¥ä¸‹æ˜¯æ›¸ç±å…§å®¹ç¯€é¸ï¼Œè«‹æ ¹æ“šå®ƒå€‘å›ç­”å•é¡Œï¼š\n\n{context}\n\nå•é¡Œï¼š{input}\n\nè«‹ç”¨ç¹é«”ä¸­æ–‡ã€æ¢ç†æ¸…æ¥šåœ°å›ç­”ã€‚"
            )

            # âœ… é€™è£¡æ˜ç¢ºå®šç¾© retriever_chainï¼Œç¢ºä¿ _format_docs æ¥æ”¶çš„æ˜¯ list ä¸¦å›å‚³ string
            def retrieve_context(question: str) -> str:
                docs = retriever.invoke(question)
                return "\n\n".join(d.page_content for d in docs)

            # âœ… RAG chain çµ„è£
            rag_chain = (
                {
                    "context": retrieve_context,
                    "input": RunnablePassthrough(),
                }
                | prompt
                | self.llm
                | StrOutputParser()
            )

            last_msg = state["messages"][-1]
            question = last_msg.content if isinstance(last_msg, HumanMessage) else ""
            answer = rag_chain.invoke(question).strip()

            print("âœ… å•ç­”å®Œæˆ")
            return {"messages": state["messages"] + [AIMessage(content=answer)]}

        # ===
        # Build graph
        # ===
        graph = StateGraph(EpubChatState)
        # add node
        graph.add_node("parse_epub", parse_epub_node)
        graph.add_node("summarize_or_index", summarize_or_index_node)
        graph.add_node("qa", qa_node)
        # add edge
        graph.add_edge(START, "parse_epub")
        graph.add_edge("parse_epub", "summarize_or_index")
        graph.add_edge("summarize_or_index", "qa")
        graph.add_edge("qa", END)
        # compile
        agent = graph.compile()

        return agent


async def main():
    aiagent_client = EpubChatAgent()
    agent = await aiagent_client.create_agent()

    config = {"configurable": {"thread_id": "epub-session-001"}, "recursion_limit": 1000}
    epub_path = "epub/test.epub"
    question = "è«‹å¹«æˆ‘æ‘˜è¦é€™æœ¬æ›¸çš„ä¸»è¦å…§å®¹èˆ‡æ ¸å¿ƒè«–é»"
    init_state = {
        "epub_path": epub_path,
        "messages": [HumanMessage(content=question)],
    }
    result = await agent.ainvoke(input=init_state, config=config)
    print(result)


if __name__ == "__main__":
    asyncio.run(main())